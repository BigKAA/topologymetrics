*[English version](noise-reduction.md)*

# Уменьшение шума

> Этот документ объясняет, как стек алертинга dephealth минимизирует шум алертов
> и помогает операторам быстро определить причину проблемы.
> Подробности о каждом правиле — в [Правила алертов](alert-rules.ru.md).
> Конфигурация Alertmanager — в [Alertmanager](alertmanager.ru.md).

---

## Проблема: усталость от алертов

Когда зависимость отказывает, наивная настройка алертинга генерирует **несколько алертов одновременно**:

- "Зависимость недоступна" (полный отказ)
- "Зависимость деградирована" (частичный отказ)
- "Высокая латентность зависимости" (падение производительности)
- "Зависимость нестабильна" (flapping)

Получить 4 алерта на один инцидент — контрпродуктивно:

- **Информационная перегрузка**: оператор тратит время на корреляцию алертов вместо починки.
- **Усталость от алертов**: увидев слишком много алертов, операторы начинают их игнорировать.
- **Неясная причина**: какой алерт — причина, а какой — симптом?

Стек алертинга dephealth решает это четырьмя механизмами: **inhibit rules**, **настройка `for` duration**, **группировка алертов** и **маршрутизация по severity**.

---

## Реальные сценарии

Каждый сценарий показывает, что происходит при конкретной проблеме — какие алерты срабатывают, какие подавляются и что должен делать оператор.

<a id="сценарий-1-бд-master-упал"></a>

### Сценарий 1: БД master упал

**Ситуация**: PostgreSQL master стал недоступен. Сервис `order-api` имеет зависимость `user-db` типа `postgres` с одним endpoint.

**Состояние метрик**:

```text
app_dependency_health{job="order-api", dependency="user-db", type="postgres", host="pg-master", port="5432"} = 0
```

**Что произойдёт без noise reduction** (4 алерта):

| Алерт | Почему сработает |
| --- | --- |
| DependencyDown | `min(...) == 0` — все endpoint-ы упали |
| DependencyHighLatency | Таймаут соединения вызывает всплеск P99 (таймаут = латентность) |
| DependencyFlapping | Если повторные попытки проверки вызывают переходы 0→1→0 |
| DependencyAbsent | НЕ срабатывает — метрики существуют, просто значение 0 |

**Что произойдёт с noise reduction** (1 алерт):

| Алерт | Статус | Причина |
| --- | --- | --- |
| **DependencyDown** | **СРАБАТЫВАЕТ** | `min == 0`, `for: 1m` истёк |
| DependencyHighLatency | **подавлен** | Inhibit: DependencyDown (critical) → все warning, equal: `[namespace, dependency]` |
| DependencyFlapping | **подавлен** | Inhibit: DependencyDown → DependencyFlapping, equal: `[job, namespace, dependency]` |

**Результат**: Оператор получает **1 алерт** — `DependencyDown (critical)` для `user-db (postgres)` в `order-api`. Причина сразу ясна: PostgreSQL master недоступен. Не нужно коррелировать несколько алертов.

**Действие оператора**: Проверить PostgreSQL master — pod запущен? PVC в порядке? Есть ли сетевая изоляция?

---

<a id="сценарий-2-одна-из-трёх-реплик-упала"></a>

### Сценарий 2: Одна из трёх реплик упала

**Ситуация**: Сервис `session-api` зависит от Redis-кластера с 3 endpoint-ами. Одна реплика стала недоступна.

**Состояние метрик**:

```text
app_dependency_health{..., dependency="cache", host="redis-0", ...} = 1  (здоров)
app_dependency_health{..., dependency="cache", host="redis-1", ...} = 0  (упал)
app_dependency_health{..., dependency="cache", host="redis-2", ...} = 1  (здоров)
```

**Что произойдёт с noise reduction**:

| Алерт | PromQL | Alertmanager | Результат |
| --- | --- | --- | --- |
| **DependencyDown** | срабатывает (`min == 0`) | **СРАБАТЫВАЕТ** (critical, не подавлен) | **оператор видит** |
| **DependencyDegraded** | срабатывает (смесь 0 и 1) | **подавлен** (inhibit: DependencyDown, equal: `[job, namespace, dependency]`) | подавлен |
| DependencyFlapping | может сработать | **подавлен** (inhibit: DependencyDown) | подавлен |

**Важный нюанс**: при одной зависимости с несколькими endpoint-ами `min == 0`, когда **любой** endpoint упал. Это означает, что DependencyDown срабатывает даже при частичном отказе. Это сделано намеренно — агрегация `min` рассматривает зависимость как целое. Если нужно различать "1 из 3 упал" и "все 3 упали" — см. [Кастомные правила](custom-rules.ru.md) для алертов per-endpoint.

**Результат**: **1 алерт** — `DependencyDown (critical)`.

**Альтернативный подход**: Если различение частичного и полного отказа важно, добавьте кастомное правило с `count` вместо `min`:

```yaml
# Срабатывает только когда ВСЕ endpoint-ы действительно упали
- alert: DependencyCompletelyDown
  expr: |
    count by (job, namespace, dependency, type) (
      app_dependency_health == 1
    ) == 0
```

См. [Кастомные правила](custom-rules.ru.md) для примеров.

**Действие оператора**: Проверить реплику Redis `redis-1` — статус pod-а, сетевая связность, использование памяти.

---

<a id="сценарий-3-http-сервис-отвечает-медленно"></a>

### Сценарий 3: HTTP-сервис отвечает медленно

**Ситуация**: Зависимость `payment-api` (тип `http`) доступна, но отвечает медленно. Health check проходит (2xx), но занимает 2-3 секунды вместо обычных 50ms.

**Состояние метрик**:

```text
app_dependency_health{..., dependency="payment-api", type="http"} = 1  (здоров — 2xx ответ)
app_dependency_latency_seconds P99 = 2.5s  (выше порога 1s)
```

**Что произойдёт с noise reduction** (1 алерт):

| Алерт | Статус | Причина |
| --- | --- | --- |
| DependencyDown | не срабатывает | `health = 1` — зависимость доступна |
| DependencyDegraded | не срабатывает | Нет endpoint-ов с `health = 0` |
| **DependencyHighLatency** | **СРАБАТЫВАЕТ** | `P99 > 1s` более 5 минут |
| DependencyFlapping | не срабатывает | Нет переключений статуса — стабильно здоров |

**Результат**: **1 алерт** — `DependencyHighLatency (warning)` для `payment-api (http)`. Другие алерты не срабатывают, т.к. зависимость доступна — просто медленна.

**Почему это правильный сигнал**: Оператор знает, что проблема — **производительность**, не **доступность**. Это сужает расследование: проверить время ответа, upstream-узкие места, использование ресурсов на payment-сервисе — а не сетевую связность или DNS.

**Действие оператора**: Расследовать производительность `payment-api` — CPU/память, запросы к БД, upstream-зависимости. Зависимость жива, значит проблема в нагрузке или ёмкости, а не в инфраструктуре.

---

<a id="сценарий-4-нестабильная-сеть-flapping"></a>

### Сценарий 4: Нестабильная сеть (flapping)

**Ситуация**: Redis-зависимость периодически становится недоступной из-за нестабильности сети. Статус здоровья чередуется: 1→0→1→0→1→0 каждые 2-3 минуты.

**Состояние метрик за 15 минут**:

```text
app_dependency_health = 1, 0, 1, 0, 1, 0, 1  (7 изменений за 15 минут)
```

**Что произойдёт с noise reduction**:

| Алерт | Статус | Причина |
| --- | --- | --- |
| DependencyDown | зависит от момента | Если проверка попадает на "down"-окно, `min == 0`. Но с `for: 1m` зависимость может восстановиться до истечения минуты → скорее всего НЕ срабатывает. |
| DependencyDegraded | не применим | Один endpoint — нет концепции частичного отказа. |
| DependencyHighLatency | маловероятно | Таймауты в "down"-периоды могут поднять P99, но фильтр `for: 5m` обычно предотвращает срабатывание. |
| **DependencyFlapping** | **СРАБАТЫВАЕТ** | `changes(health[15m]) = 7 > 4` — срабатывает немедленно (`for: 0m`). |

**Результат**: **1 алерт** — `DependencyFlapping (info)`. `$value` в описании показывает "7 раз" — оператор знает, что это нестабильность, а не жёсткий отказ.

**Почему `info` severity важен**: Flapping-алерты по умолчанию идут в null receiver (только UI Alertmanager, без push-нотификаций). Это намеренно:

- Flapping — **симптом**, а не причина. Оператор должен расследовать *почему* зависимость нестабильна.
- Push-нотификации для flapping добавляют шум — оператор уже знает о проблеме, если DependencyDown срабатывает в "down"-окне.

**Действие оператора**: Расследовать стабильность сети — проверить потерю пакетов, разрешение DNS, правила firewall, исчерпание ресурсов на Redis-ноде.

---

<a id="сценарий-5-перезапуск-сервиса-или-деплой"></a>

### Сценарий 5: Перезапуск сервиса или деплой

**Ситуация**: Rolling deployment заменяет все pod-ы сервиса `catalog-api`. Во время раскатки есть 3-минутное окно, когда ни один pod не экспортирует dephealth-метрики.

**Состояние метрик**:

```text
# До деплоя: метрики есть
app_dependency_health{job="catalog-api", ...} = 1

# Во время деплоя (3 минуты): метрик нет вообще
# (старый pod завершён, новый ещё не готов)

# После деплоя: метрики вернулись
app_dependency_health{job="catalog-api", ...} = 1
```

**Что произойдёт с noise reduction**:

| Алерт | Статус | Причина |
| --- | --- | --- |
| DependencyDown | не срабатывает | Нет данных → нет вычисления `min == 0`. `absent` ≠ `== 0`. |
| DependencyDegraded | не срабатывает | Нет данных для вычисления. |
| DependencyHighLatency | не срабатывает | Нет данных для вычисления. |
| DependencyFlapping | не срабатывает | Нет данных для вычисления. |
| DependencyAbsent | зависит от длительности | `for: 5m` — если деплой занимает менее 5 минут, алерт НЕ срабатывает. Если дольше 5 минут → **СРАБАТЫВАЕТ**. |

**Результат при деплое < 5 минут**: **0 алертов**. Фильтр `for: 5m` поглощает разрыв. Ожидаемое поведение для нормальных деплоев.

**Результат при деплое > 5 минут**: **1 алерт** — `DependencyAbsent (warning)`. Сигнализирует о потенциальной проблеме деплоя — раскатка идёт слишком долго, или новые pod-ы не стартуют.

**Почему это работает**: Порог 5 минут настроен на типичные Kubernetes rolling update. Большинство деплоев завершаются за 2-3 минуты. Если метрики отсутствуют дольше — что-то не так.

**Действие оператора**: Проверить статус деплоя — `kubectl rollout status`, события pod-ов, статус загрузки образов.

---

<a id="inhibit-rules"></a>

## Механизм 1: Inhibit rules

Inhibit rules — способ Alertmanager **подавлять менее важные алерты, когда более серьёзный алерт уже сработал** для той же зависимости.

### Иерархия подавления

```text
DependencyDown (critical)
  ├── подавляет → DependencyDegraded    (equal: job, namespace, dependency)
  ├── подавляет → DependencyHighLatency (equal: job, namespace, dependency)
  ├── подавляет → DependencyFlapping    (equal: job, namespace, dependency)
  └── подавляет → все warning severity  (equal: namespace, dependency)

DependencyDegraded (warning)
  └── подавляет → DependencyFlapping    (equal: job, namespace, dependency)
```

### Как работают `equal` labels

Поле `equal` указывает, какие labels должны совпадать для применения подавления:

```yaml
inhibit_rules:
  - source_matchers:
      - alertname = "DependencyDown"
    target_matchers:
      - alertname = "DependencyDegraded"
    equal: ['job', 'namespace', 'dependency']
```

Это означает: "Если DependencyDown сработал для `job=order-api, namespace=production, dependency=user-db`, подавить DependencyDegraded **только если** у него те же значения `job`, `namespace` и `dependency`."

**Почему это важно**: Если `order-api` имеет DependencyDown для `user-db` и DependencyDegraded для `cache`, подавляется только DependencyDegraded для `user-db`. Алерт для `cache` по-прежнему срабатывает — это другая зависимость.

### Каскадное правило

Последнее inhibit rule шире:

```yaml
- source_matchers:
    - alertname = "DependencyDown"
    - severity = "critical"
  target_matchers:
    - severity = "warning"
  equal: ['namespace', 'dependency']
```

Обратите внимание: `equal` использует только `namespace` и `dependency` (без `job`). Это подавляет warning-алерты **во всех сервисах**, зависящих от той же упавшей зависимости. Если `user-db` упал — все сервисы, зависящие от `user-db`, получают подавление warning-алертов. Остаются только critical DependencyDown.

---

## Механизм 2: `for` duration

Поле `for` управляет тем, как долго условие должно быть истинным, прежде чем алерт сработает. Это первая линия защиты от транзитных сбоев.

| Алерт | `for` | Обоснование |
| --- | --- | --- |
| DependencyDown | 1m | Критический отказ — алерт быстро, но отфильтровать 1-2 плохих scrape (интервал = 15s) |
| DependencyDegraded | 2m | Частичный отказ часто временный (rolling update, перепланирование pod-ов) |
| DependencyHighLatency | 5m | Латентность флуктуирует естественно (GC, прогрев пула, всплески нагрузки) |
| DependencyFlapping | 0m | `changes()` уже использует 15-минутное окно — дополнительная задержка не нужна |
| DependencyAbsent | 5m | Покрывает типичные деплой-разрывы (rolling update занимает 2-3 минуты) |

### Что будет без `for`

Если у всех правил `for: 0m`:

- **DependencyDown**: Каждый кратковременный сетевой сбой вызывает critical-page. С 15s scrape interval один неудачный scrape = алерт.
- **DependencyHighLatency**: Одна GC-пауза или холодное соединение вызывает всплеск P99 → ложный алерт.
- **DependencyAbsent**: Каждый rolling deployment вызывает алерт → усталость от алертов.

### Настройка `for` под ваше окружение

- **Быстрее обнаружение** (короче `for`): уменьшите значения, если SLO требует быстрой реакции. Компромисс: больше ложных срабатываний.
- **Меньше шума** (длиннее `for`): увеличьте значения, если у вас много транзитных сбоев. Компромисс: медленнее обнаружение реальных проблем.

---

## Механизм 3: Группировка алертов

Alertmanager группирует связанные алерты в одно уведомление, уменьшая количество сообщений оператору.

### Конфигурация группировки по умолчанию

```yaml
route:
  group_by: ['alertname', 'namespace', 'job', 'dependency']
  group_wait: 30s
  group_interval: 5m
```

| Параметр | Значение | Эффект |
| --- | --- | --- |
| `group_by` | `[alertname, namespace, job, dependency]` | Алерты с одинаковой комбинацией этих labels группируются вместе |
| `group_wait` | 30s | После первого алерта в группе ждём 30s для остальных |
| `group_interval` | 5m | Минимальное время между уведомлениями для одной группы |

**Почему `group_by` НЕ включает `host` или `port`**:

Если бы `host` был в `group_by`, каждый endpoint генерировал бы отдельное уведомление. Для Redis-кластера с 3 endpoint-ами — 3 отдельных сообщения вместо 1.

При группировке по `dependency` все endpoint-ы одной зависимости объединяются: "Зависимость `cache` — 2 из 3 endpoint-ов упали" — одно сообщение.

### Critical-алерты: быстрая доставка

```yaml
routes:
  - matchers:
      - severity = "critical"
    group_wait: 10s
    repeat_interval: 1h
```

Для critical-алертов `group_wait` сокращён до 10s (вместо 30s по умолчанию) для быстрой доставки. `repeat_interval` тоже короче (1h вместо 4h) для поддержания осведомлённости оператора.

---

## Механизм 4: Маршрутизация по severity

Разные уровни severity направляются в разные receivers:

| Severity | Receiver | Поведение |
| --- | --- | --- |
| `critical` | default (webhook/Telegram) | Push-нотификация, `group_wait: 10s`, `repeat_interval: 1h` |
| `warning` | default (webhook/Telegram) | Push-нотификация, `group_wait: 30s`, `repeat_interval: 4h` |
| `info` | null | Только UI Alertmanager, без push-нотификаций |

### Почему `info` идёт в null

Info-алерты (DependencyFlapping) — диагностические сигналы. Push-нотификации для них:

1. Увеличивают шум — flapping часто сопровождает другие алерты.
2. Не побуждают к действию — "зависимость нестабильна" требует расследования, а не немедленных действий.
3. Всё равно подавляются — inhibit rules подавляют flapping при срабатывании DependencyDown или DependencyDegraded.

Info-алерты по-прежнему видны в UI Alertmanager для операторов, активно расследующих инциденты.

---

## Антипаттерны: что НЕ делать

### 1. `for: 0m` на critical-алертах

```yaml
# ПЛОХО: срабатывает на каждый транзитный сбой
- alert: DependencyDown
  expr: min by (...) (app_dependency_health) == 0
  for: 0m   # ← шторм нотификаций
```

С 15s scrape interval одна сетевая помеха вызывает critical-page. Оператора будят в 3 часа ночи из-за проблемы, которая решилась через 15 секунд.

### 2. Отсутствие inhibit rules

Без inhibit rules один отказ PostgreSQL генерирует:

- DependencyDown (critical) — корректно
- DependencyHighLatency (warning) — шум (таймаут = высокая латентность)
- DependencyFlapping (info) — шум (если повторные попытки вызывают переходы)

Оператор получает 3 алерта и должен коррелировать их вручную. При 10 зависимостях это становится неуправляемым.

### 3. `group_by` с instance/host/port

```yaml
# ПЛОХО: каждый endpoint генерирует отдельное уведомление
group_by: ['alertname', 'namespace', 'job', 'dependency', 'host', 'port']
```

Для зависимости с 5 endpoint-ами — 5 отдельных сообщений вместо 1. Telegram оператора заполняется сообщениями об одной и той же проблеме.

### 4. Слишком короткий `repeat_interval`

```yaml
# ПЛОХО: повторяет каждые 5 минут для неразрешённых алертов
repeat_interval: 5m
```

Если зависимость недоступна 1 час — оператор получает 12 повторных нотификаций. Самый быстрый способ заставить операторов отключить канал.

**Рекомендация**: `repeat_interval: 4h` для warning, `1h` для critical. Оператор знает о проблеме — повторять каждые 5 минут не помогает.

### 5. Один receiver для всех severity

```yaml
# ПЛОХО: info/warning/critical идут в один Telegram-канал
route:
  receiver: ops-telegram
  # нет маршрутов по severity
```

Info-алерты (flapping) заполняют тот же канал, что и critical (DependencyDown). Critical-алерты теряются в шуме.

**Решение**: Critical → канал с высоким приоритетом (со звуковыми нотификациями), warning → обычный канал, info → null или низкоприоритетный канал.

---

## Итого: как всё работает вместе

При возникновении проблемы четыре механизма работают последовательно:

```text
1. PromQL-выражение вычисляется как true
   │
2. Фильтр `for` duration: транзитные проблемы отфильтрованы
   │
3. Алерт отправлен в Alertmanager
   │
4. Inhibit rules: избыточные алерты подавлены
   │
5. Группировка: связанные алерты объединены
   │
6. Маршрутизация: направлено в нужный receiver по severity
   │
7. Оператор получает: 1 понятное, действенное уведомление
```

Цель: **одна проблема → один алерт → одно понятное действие**.

---

## Использование метрик статуса для анализа причин

Начиная с v0.4.0, dephealth экспортирует две дополнительные метрики: `app_dependency_status` (категория статуса) и `app_dependency_status_detail` (детальная причина). Эти метрики помогают быстрее определить причину сбоя без написания кастомных правил.

### Как метрики статуса дополняют алертинг

Когда `DependencyDown` срабатывает, оператор знает **что** упало, но не **почему**. Метрики статуса дают ответ на «почему»:

| Сценарий | `app_dependency_health` | `app_dependency_status` (серия = 1) | `app_dependency_status_detail` |
| --- | --- | --- | --- |
| БД недоступна | 0 | `connection_error` | `connection_refused` |
| Ошибка аутентификации БД | 0 | `auth_error` | `password_authentication_failed` |
| HTTP 503 | 0 | `response_error` | `http_503` |
| Истёк TLS-сертификат | 0 | `connection_error` | `certificate_has_expired` |
| Ошибка DNS-разрешения | 0 | `connection_error` | `no_such_host` |
| Таймаут Redis | 0 | `timeout` | `context_deadline_exceeded` |

### Пример: Сценарий 1 с метриками статуса

Возвращаясь к [Сценарию 1 (БД master упал)](#сценарий-1-бд-master-упал), оператор получает `DependencyDown (critical)`. Для определения причины запросите метрики статуса:

```promql
# Какая категория статуса?
app_dependency_status{job="order-api", dependency="user-db", status!=""} == 1
# Результат: status="connection_error" → 1

# Какая детальная причина?
app_dependency_status_detail{job="order-api", dependency="user-db", detail!=""} == 1
# Результат: detail="connection_refused" → 1
```

Оператор сразу понимает: PostgreSQL отказывает в соединении (не ошибка аутентификации, не таймаут, не проблема DNS). Это сужает расследование до: запущен ли процесс? Открыт ли порт? Доступен ли сетевой путь?

### Интеграция с Grafana

Добавьте эти запросы в Grafana-дашборды для мгновенной видимости причин:

```promql
# Категория статуса зависимости (используйте в Stat или Table панели)
app_dependency_status{status!=""} == 1

# Детальная причина (используйте в Table панели)
app_dependency_status_detail{detail!=""} == 1
```

Для кастомных правил алертов с метриками статуса см. [Кастомные правила — Пример 6](custom-rules.ru.md#example-6-status-category-alerts).

---

## Что дальше

- [Правила алертов](alert-rules.ru.md) — подробный разбор PromQL каждого правила
- [Конфигурация Alertmanager](alertmanager.ru.md) — как настроить маршрутизацию, receivers и inhibit rules
- [Кастомные правила](custom-rules.ru.md) — написание своих правил с правильным noise reduction
